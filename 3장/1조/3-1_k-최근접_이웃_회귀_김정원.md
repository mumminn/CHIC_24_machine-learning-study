## k-최근접 이웃 회귀
### 회귀
- 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제
ex. 내년도 경제 성장률 예측, 배달 도착 시간 예측
#### k-최근접 이웃 분류 알고리즘
1. 예측하려는 샘플에 가장 가까운 샘플 k개 선택
2. 선택한 샘플들의 클래스 확인
3. 다수 클래스를 샘플의 클래스로 예측  
*ex. k=3일 때,주변 샘플 3개 탐색 -> 2개가 A, 1개가 B인 경우, 이 샘플은 A로 분류*
#### k-최근접 이웃 회귀 알고리즘
1. 예측하려는 샘플에 가장 가까운 샘플 k개 선택
2. 선택한 샘플들의 수치 확인
3. 이 수치들의 평균을 샘플의 수치로 예측
### 데이터 준비
- 배열 생성 및 산점도 그리기
  ```
  import matplotlib.pyplot as plt
  plt.scatter(perch_length, perch_weight)
  plt.xlabel('length')
  plt.ylabel('weight')
  plt.show()
  ```
  - 길이에 따라 무게가 증가함을 확인
- 훈련 세트와 테스트 세트로 분리
  ```
  from sklearn.model_selection import train_test_split
  train_input, test_input, tarin_target, test_target = train_test_split(perch_length, perch_weight, rondom_state=42)
  ```
### 결정계수(R<sup>2</sup>)
```
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()

# k-최근접 이웃 회귀 모델 훈련
knr.fit(trian_input, train_target)

print(knr.score(test_input, test_target))
```
R<sup>2</sup>= 1 - { (타깃-예측<sup>2</sup>의 합) / (타깃-평균<sup>2</sup>의 합) }
- 각 샘플의 타깃과 예측한 값의 차이를 제곱하여 더함
- 타깃과 타깃 평균의 차이를 제곱하여 더한 값으로 나눔
- 타깃의 평균 정도를 예측한다면 R<sup>2</sup>는 0에 가까워짐
- 예측이 타깃에 아주 가까워지면 1에 가까워짐
### 과대적합 vs 과소적합
#### 과대적합
- 훈련 세트에만 잘 맞게 학습된 모델
- 테스트 세트 혹은 실제 적용 시 잘 동작하지 않음
#### 과소적합
- 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수 모두 낮은 경우
- 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우
- 훈련 세트가 전체 데이터를 대표한다고 가정하기 때문에 훈련 세트를 잘 학습하는 것이 중요
- 훈련 세트/테스트 세트 크기가 매우 작은 경우 과소적합 발생
#### 과소적합 해결
- 모델을 더 복잡하게 만들어야 함
- k-최근접 이웃 알고리즘에서는 이웃의 개수 k를 줄여 모델을 복잡하게 만듦
  - 이웃의 개수를 줄이면 훈련 세트에 있는 국지적 패턴에 민감해짐
  - 이웃의 개수를 늘리면 데이터 전반에 있는 일반적 패턴을 따름
  ```
  # 이웃 개수 3으로 설정
  knr.n_neighbors = 3
  ```
